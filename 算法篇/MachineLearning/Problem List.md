# Problem List

## 1. 逻辑回归为什么用极大似然函数作为损失函数，而不用平方损失？

- 平方损失关于参数是非凸的

  平方损失函数加上sigmoid的函数将会是一个非凸的函数，不易求解，会得到局部解，用对数似然函数得到高阶连续可导凸函数，可以得到最优解。

- 平方损失更新慢

  对数损失函数更新起来很快，因为只和x，y有关，和sigmoid本身的梯度无关，而如果使用平方损失，梯度更新的速度会和sigmoid函数的梯度相关，**sigmoid 函数在定义域内的梯度都不大于0.25**，导致训练速度会非常慢，证明如下：

  假设预测$f(x_i)=w^Tx_i + b$，

  如果选用均方误差作为损失，损失为$\sum_{i=1}^n[y_i-\sigma(f(x_i))]^2$，其关于参数$w_j$的梯度为$(y_i-\sigma(f(x_i)))\cdot \sigma^{'}(f(x_i)) \cdot x_i^{(j)}$，其中$\sigma^{'}$位于(0, 0.25)之间，且如果输入的数较大及容易落入饱和区，即其导数易为0，所以说训练速度较慢。

  如果选用对数损失，损失为$-\sum_{i=1}^n y_i \cdot \ln[\sigma(f(x_i))]+(1-y_i) \cdot \ln[1 - \sigma(f(x_i))]$，其关于参数$w_j$的梯度为$(y_i-\sigma(f(x_i)))\cdot x_i^{j}$，其梯度中相比较于均方误差的梯度，恰好少了导数项，所以收敛速度是快于均方误差的。

> 往往使用平方损失假设目标变量服从高斯分布，而使用极大似然（对数损失）假设目标变量服从伯努力分布

## 2. 逻辑回归模型的优缺点分析？

- 优点
  - 形式简单，模型的可解释性非常好。从特征的权重可以看到不同的特征对最后结果的影响，某个特征的权重值比较高，那么这个特征最后对结果的影响会比较大。
  - 模型效果不错。在工程上是可以接受的（作为baseline)，如果特征工程做的好，效果不会太差，并且特征工程可以大家并行开发，大大加快开发的速度。
  - 训练速度较快。分类的时候，计算量仅仅只和特征的数目相关。并且逻辑回归的分布式优化sgd发展比较成熟，训练的速度可以通过堆机器进一步提高，这样我们可以在短时间内迭代好几个版本的模型。
  
  > 主要就是两点，解释性好+训练速度快
- 缺点
  - 准确率并不是很高。因为形式非常的简单(非常类似线性模型)，很难去拟合数据的真实分布。
  - 处理非线性数据较麻烦。逻辑回归在不引入其他方法的情况下，只能处理线性可分的数据，或者进一步说，处理二分类的问题 。
  
  > 主要就是效果相对较差

## 3. LR和SVM的联系和区别？

联系：

- 两者均支持分类任务（SVM也支持回归任务）
- 如果不考虑核函数，两者均是线性分类算法，他们的分类决策面均是线性。
- 两个均属于判别式模型

区别：

- 两者的loss不同（最核心的区别）

  - LR为Logistic Loss($\log (1 + e^{-z})$)
    $$
    \begin{aligned}
    L(y, \hat{y}) &= \sum_{i=1}^m y_i \log \hat{y}_i + (1-y_i) \log (1-\hat{y}_i)\\
    &= \sum_{i=1}^m y_i \log \frac{1}{1+e^{-(w^Tx_i+b)}}+(1-y_i)( 1 - \frac{1}{1+e^{-(w^Tx_i+b)}}) \\
    &= \sum_{i=1}^m \log(1 + e^{-y_i(w^Tx_i+b)})
    \end{aligned}
    $$

  - SVM为hinge Loss($\max(0, 1-z)$)
    $$
    L(y, \hat{y})= \frac{1}{2} ||w||^2 + C\sum_{i=1}^m \max(0, 1-y_i(w^Tx_i+b))
    $$

  两者最本质的不同即为loss函数选择的不同，不过从算法的出发角度来看，也是大不一样的，其中LR是假设样本为1的概率可以用sigmoid函数来表示，然后通过**极大似然估计**的方法估计出参数的值；而**支持向量机基于几何间隔最大化原理**，认为存在最大几何间隔的分类面为最优分类面

  > 从两者的目标形式来看，SVM相当于自带L2正则项（该部分本表示支持向量的间隔），**这就是为什么SVM是结构风险最小化算法的原因**，而LR则需要添加L2正则。
  >
  > 结构风险最小：意思就是在训练误差和模型复杂度之间寻求平衡，防止过拟合，从而达到真实误差的最小化。

- 对数据的依赖

  由于损失函数选择的不同，**支持向量机只考虑局部的边界线附近的点，而逻辑回归考虑全局（远离的点对边界线的确定也起作用，虽然作用会相对小一些）**

  SVM决策面的样本点只有少数的支持向量，当在支持向量外添加或减少任何样本点对分类决策面没有任何影响；LR中，每个样本点都会影响决策面的结果。

  由此得知，**线性SVM不直接依赖于数据分布，分类平面不受非支持向量点影响；LR则受所有数据点的影响，如果数据不同类别strongly unbalance，一般需要先对数据做balancing**

- 非线性问题的解决

  **在解决非线性问题时，支持向量机采用核函数的机制，而LR通常不采用核函数的方法**

  这个问题理解起来非常简单。分类模型的结果就是计算决策面，模型训练的过程就是决策面的计算过程。通过上面的第二点不同点可以了解，在计算决策面时，SVM算法里只有少数几个代表支持向量的样本参与了计算，也就是只有少数几个样本需要参与核计算。然而，LR算法里，每个样本点都必须参与决策面的计算过程，也就是说，假设我们在LR里也运用核函数的原理，那么每个样本点都必须参与核计算，这带来的计算复杂度是相当高的。所以，在具体应用时，LR很少运用核函数机制

- **线性SVM依赖数据表达的距离测度，所以需要对数据先做normalization，LR不受其影响**

  **一个基于概率，一个基于距离！**

## 4. 判别模型和生成模型

- 判别模型

  **判别式模型**（Discriminative Model）是直接对条件概率p(y|x;θ)建模。学习得到条件概率分布P(y|x)，即在特征x出现的情况下标记y出现的概率。

  常见的判别式模型有线性回归模型、线性判别分析、支持向量机SVM、神经网络、boosting、条件随机场等。

- 生成式模型

  **生成式模型**（Generative Model）则会对x和y的联合分布p(x,y)建模，然后通过贝叶斯公式来求得p(yi|x)，然后选取使得p(yi|x)最大的yi。

  常见的生成式模型有 隐马尔可夫模型HMM、朴素贝叶斯模型、高斯混合模型GMM

> 举例：
>
> - 判别模型
>
>   要确定一个羊是山羊还是绵羊，用判别模型的方法是从历史数据中学习到模型，然后通过提取这只羊的特征来预测出这只羊是山羊的概率，是绵羊的概率。
>
> - 生成模型
>
>   利用生成模型是根据山羊的特征首先学习出一个山羊的模型，然后根据绵羊的特征学习出一个绵羊的模型，然后从这只羊中提取特征，放到山羊模型中看概率是多少，在放到绵羊模型中看概率是多少，哪个大就是哪个。
>

## 5. SVM的基本原理？

SVM是一种二类分类模型。它的基本模型是**在特征空间中寻找间隔最大化的分离超平面**的线性分类器。

（1）当训练样本线性可分时，通过硬间隔最大化，学习一个线性分类器，即线性可分支持向量机；

（2）当训练数据近似线性可分时，引入松弛变量，通过软间隔最大化，学习一个线性分类器；

（3）当训练数据线性不可分时，通过使用核技巧及软间隔最大化，学习非线性支持向量机。

## 6. SVM为什么采用间隔最大化？

当训练数据线性可分时，存在无穷个分离超平面可以将两类数据正确分开。

线性可**分支持向量机利用间隔最大化求得最优分离超平面**，这时，解是唯一的。另一方面，此时的分隔超平面所产生的分类结果是最鲁棒的，**对未知实例的泛化能力最强**。

## 7. 为什么要将求解SVM的原始问题转换为其对偶问题？

一、是对偶问题往往更易求解（当我们寻找约束存在时的最优点的时候，约束的存在虽然减小了需要搜寻的范围，但是却使问题变得更加复杂。为了使问题变得易于处理，我们的方法是把目标函数和约束全部融入一个新的函数，即拉格朗日函数，再通过这个函数来寻找最优点。）

二、自然引入核函数，进而推广到非线性分类问题。

## 8. 为什么SVM要引入核函数？

当样本在原始空间线性不可分时，可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分。

如果不引入核函数，有可能特征会映射到无穷维度，这样在做特征内积时会非常耗时，通常通过核函数来对其近似。

## 9. SVM的RBF核函数的具体公式？

$\kappa(x_i, x_j) = exp(-\frac{||x_i - x_j||^2}{2\sigma^2})$，RBF核亦称为高斯核，这个核会将原始空间映射为无穷维空间，

如果$\sigma$选得很大的话，高次特征上的权重实际上衰减得非常快，所以实际上（数值上近似一下）相当于一个低维的子空间；反过来，如果$\sigma$选得很小，则可以将任意的数据映射为线性可分——当然，这并不一定是好事，因为随之而来的可能是非常严重的过拟合问题。不过，总的来说，通过调控参数$\sigma$, 高斯核实际上具有相当高的灵活性，也是使用最广泛的核函数之一。

## 10. 介绍一下LR？

LR是一个二分类模型，它假设数据标签服从伯努力分布，然后通过对特征的线性加权和sigmoid的变换表示出每个样本的概率，通过极大化似然函数的方法，通过梯度下降等优化算法进行参数的求解，从而达到二分类的目的，当然可以将LR扩展至多分类。

## 11. 介绍一下SVM？

同问题5。

## 12. 介绍一下决策树？

决策树是一种基本的分类与回归的方法，可以理解决策树是if-else一系列集合，树的内部对应决策的条件，而树的叶子则对应决策的结论。目前来说决策树主要有ID3, C4.5和CART这三种模型。

## 13. 介绍一下RF？

它属于Baaging算法框架，它并行地学习多个决策树模型（每个决策树的学习互不影响），最终将每个决策树的预测结果通过投票或者取均值的方式得到最终的预测。

## 14. 介绍一下GBDT？

它属于Boosting算法框架，它串行地学习多个决策树模型，每个决策树模型学习完成后，然后基于目标函数更新整体的学习目标，下一个决策树模型学习此目标，依次类推，直到达到中止条件，最终将每个决策树模型结果汇总得到最终预测。

## 15. 介绍一下XGBoost？

它属于Boosting算法框架，它对目标函数进行二阶泰勒展开，然后将决策树模型融入，推导得出决策树的最优叶子得分以及对应的特征分割点对应的增益方式，初此之外增加了多种分割点选择策略，增加了对缺失数据的处理，以及整体系统架构的优化。

最终学习方式和boosting框架一致，串行地学习多个决策树模型，然后基于目标函数更新整体的学习目标，然后一个决策树模型学习此目标，依次类推，直到达到中止条件，最终将每个决策树模型结果汇总得到最终的预测。

## 16. 介绍一下LightGBM？

它属于Boosting算法框架，它串行地学习多个决策树模型，在学习时做了一些列的优化，采用了GOSS和EFB策略，在确保精确度的前提下，极大削减了数据量，加快了模型的学习速度，除此之外，还有其它的优化，包括分割点采用直方图方法进行学习，决策树由level-wise方式变为leaf-wise方式，增加了类别类型特征的处理，以及整体系统架构的优化。

最终学习方式和boosting框架一致，串行地学习多个决策树模型，然后基于目标函数更新整体的学习目标，然后一个决策树模型学习此目标，依次类推，直到达到中止条件，最终将每个决策树模型结果汇总得到最终的预测。

## 17. 介绍一下Catboost？

它属于Boosting算法框架，其主要特点是对类别行变量做了极大支持，且创新地提出了一种方式来解决**梯度偏移**的问题。

最终学习方式和boosting框架一致，串行地学习多个决策树模型，然后基于目标函数更新整体的学习目标，然后一个决策树模型学习此目标，依次类推，直到达到中止条件，最终将每个决策树模型结果汇总得到最终的预测。

## 18. RF和GBDT比较？

- 相同点

  均是集成学习算法的代表，均可以做分类和回归任务，两者效果均可以取得不错的效果。

- 不同点

  RF是Bagging算法框架，它是并行地学习多个不同的决策树模型，每个模型之间互不影响。

  GBDT是Boosting算法框架，它是串行地学习多个不同的决策树模型，每个决策树模型学习完成后，基于当前的学习通过目标函数更新学习目标，然后下一个决策树模型基于此学习目标进行学习，以此类推。

## 19. GBDT和XGBoost比较？

- 相同点

  均是Boosting算法框架的代表算法，两者均可以做分类和回归任务

- 不同点

  XGBoost在目标函数中添加了正则项，且对目标函数做了二阶泰勒展开，重新定义了决策树叶子得分和决策树分割点增益函数，增加了对缺失数据的处理，在系统架构上做了很多优化，整体运行速度上较GBDT有极大的提升，模型效果也比GBDT有很大的提升。

## 20. XGBoost和LightGBM比较？

- 相同点

  均是Boosting算法框架的代表算法，两者均可以做分类和回归任务

- 不同点

  - 整体思想框架层面

    XGBoost更侧重在决策树方面的优化，包括对目标函数二阶泰勒展开，将决策树模型融入进去从而得到最优的决策树叶子得分以及对应新的的决策树分割点选择的增益函数，还有对缺失数据的支持

    LightGBM更侧重在整体工程的应用（确保精度，提升运行效率），主要它提出的两种算法GOSS和EFB，在确保模型精度下，一个是侧重减少样本数量，一个是侧重减少特征数量。

  - 细节优化层面

    LightGBM相较于XGBoost更多一些，LightGBM采用Histogram方式来寻找最优分割点，XGBoost则需要对特征进行预排序；决策树的生长方式上，XGBoost是level-wise方式，而LightGBM采用了leaf-wise方式；XGBoost不支持类别类型特征，而LightGBM则支持；在系统架构优化上两者均做了大量的优化，最终运行速度上，在精度相近的境况下，LightGBM要显著快于XGBoost。

## 21. CatBoost和XGB/LGB比较？

- 相同点

  三者都是Booting算法框架的典型代表算法，目前三者均是业界最常用分类算法。

- 不同点

  主要从CatBoost层面来看，

  - 类别类型特征的支持

    XGBoost算法不支持类别类型特征，LightGBM支持类别类型特征，将类别特征转换为连续值进行处理，而CatBoost以打乱样本且增加平滑处理的方式将类别特征转换为连续值，且考虑了特征组合，包括类别和类别之间组合以及类别和连续特征之间的组合。

    > 显然CatBoost对类别特征处理要更加优于XGB和LGB

  - gradient bias

    在CatBoost论文中之处，boosting算法框架在每一步更新学习目标时存在gradient bias，其提出了一种修正的方式，通过打乱样本，每次学习n（样本量）个模型，然后基于此进行梯度的修正来更新学习目标，但是每次学习n个模型，显然学习成本及其之大。

> 所以整体来看，CatBoost提出的gradient bias修正方式虽然可行，但是及其耗时，基本不太现实，所以其核心优化点则是集中在类别特征的处理上，从这点来看话，其整体优化还是偏少，目前来说相较于XGB和LGB其使用率在业界还是偏低的。

## References

- [逻辑回归常见面试点总结！！！](https://zhuanlan.zhihu.com/p/76563562)

- [LR和SVM的区别](https://www.cnblogs.com/always-fight/p/9221783.html)

- [数据挖掘（机器学习）面试--SVM面试常考问题](https://blog.csdn.net/szlcw1/article/details/52259668)

